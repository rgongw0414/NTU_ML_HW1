# -*- coding: utf-8 -*-
"""NTU_ML_HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NRIzmBTg3lvNRg7KqAHeYpHOhLeN-vkf
"""

import numpy as np
import math
import csv
import time
from google.colab import drive
np.set_printoptions(suppress=True) 
drive.mount('/content/drive')

cd drive/My Drive/CSIE/Machine Learning

pwd

raw_data = np.genfromtxt('NUT - ML_DL/train.csv', delimiter=',', encoding='big5') ## load train.csv
data = raw_data[1:, 3:]  # data[10]為PM2.5

where_are_NaNs = np.isnan(data)
data[where_are_NaNs] = 0.0   # 將nan轉為數值0.0
  
x = np.zeros((1, 24*20*12))  
for row in range(18):  # 把所有日子的氣象資料整理為形狀(18, -1)的矩陣
  data_term = np.array(data[row, :].reshape(1, 24))
  while(row+18 < len(data)):
    row += 18    
    data_term = np.append(data_term, data[row,  : ].reshape(1, 24), axis = 1) 

  x = np.append(x, data_term, axis = 0)
x = np.delete(x, 0, 0)

a = np.array([1, 2, 3])
np.append(a, 4)
print(a)

for a in where_are_NaNs:
  for b in a:
    if b == True:
      print(b.where)

train_x = [ ]
train_y = [ ]

for month in range(12): 
  for day in range(20):    
    for hr in range(24):   
      if(day != 19) or (hr < 15): # 撇除無法用來預測的data
        # 將每九個小時的十八種類別資料，存為一筆一維陣列，每個月有480筆資料(每個月20天，每天24筆)
        train_x.append(x[ : , (month * 480 + day * 24 + hr) : (month * 480 + day * 24 + hr + 9)])

      if(day != 0) or (hr > 8):  # 撇除無法作為被預測的data（第十小時）
        train_y.append(x[9, (month * 480 + day * 24 + hr)])  # 可用前九小時的data來預測的PM2.5

print('shape of train_x :', np.array(train_x).shape)
print('shape of train_y(10-th hour PM2.5) :', np.array(train_y).shape)
print('model : y = b + X*w')
print('Loss function : Loss = summation(y\' - y)^2' )

train_x = np.array(train_x)
train_x = train_x.reshape(train_x.shape[0], -1) # rehshape into (5652, 18 *9)
train_y = np.array(train_y)
train_y = train_y.reshape(train_y.shape[0], -1) # rehshape into (5652, 1)

# Nolmalization
mean = np.mean(train_x, axis = 0)
std = np.std(train_x, axis = 0)
for i in range(train_x.shape[0]):
  for j in range(train_x.shape[1]):
    if not std[j] == 0 :
      train_x[i][j] = (train_x[i][j]- mean[j]) / std[j]

b = 1.0
w = np.zeros((train_x.shape[1], 1))

b_history = np.array(b)
w_history = np.array(w.reshape(1, -1))

b_grad_history = b_history
w_grad_history = w_history

# using AdaGrad
lr = 1.0   # learning rate
lr_b = 1.0  # customized learning rate for b
lr_w = 1.0  # customized learning rate for w
iteration = 10000

t_start = time.time()
for i in range(iteration):      
  b_grad = np.sum((-2.0) * (train_y - b - train_x.dot(w)))  # train_x.shape[0] = 5652
  w_grad = (-2.0) * (np.transpose(train_x).dot(train_y - b - train_x.dot(w)))
  
  lr_b += b_grad ** 2.0
  lr_w += w_grad ** 2.0
  
  b -= lr * b_grad / (np.sqrt(lr_b) + 0.00000001)
  w -= lr * w_grad / (np.sqrt(lr_w) + 0.00000001)
  
  b_history = np.append(b_history, b)
  w_history = np.append(w_history, w.reshape(1, -1), axis = 0)
  
  b_grad_history = np.append(b_grad_history, b_grad)
  w_grad_history = np.append(w_grad_history, w_grad.reshape(1, -1), axis = 0)
  
  if(i % 500 == 499):
    t_end = time.time()
    print('-\nIteration[%d]:\nb = %.9f, b_grad = %.9f, mean of w = %.6f, mean of w_grad = %.6f' % (i + 1, b, b_grad, np.mean(w), np.mean(w_grad)))    
    print("Loss:", np.power(np.sum(np.power(train_x.dot(w) - b - train_y, 2 ))/ train_x.shape[0], 0.5))
    #print("Loss:", np.sum(np.power(train_y - b - train_x.dot(w), 2 )))
    print('Elapsed time: %.2f min.' % ((t_end - t_start) / 60.0))

# save the training result into .csv files
with open('NUT - ML_DL/b_history.csv', 'w', newline='') as csvfile:  
  writer = csv.writer(csvfile)  # 建立 CSV 檔寫入器
  writer.writerow(b_history)    # 寫入一列資料

with open('NUT - ML_DL/w_history.csv', 'w', newline='') as csvfile:  
  writer = csv.writer(csvfile)
  writer.writerows(w_history)

with open('NUT - ML_DL/b_grad_history.csv', 'w', newline='') as csvfile:  
  writer = csv.writer(csvfile)
  writer.writerow(b_grad_history)

with open('NUT - ML_DL/w_grad_history.csv', 'w', newline='') as csvfile:  
  writer = csv.writer(csvfile)
  writer.writerows(w_grad_history)

# load the training result
b_result = np.genfromtxt('NUT - ML_DL/b_history.csv', delimiter=',', encoding='big5')
b_grad_result = np.genfromtxt('NUT - ML_DL/b_grad_history.csv', delimiter=',', encoding='big5')
w_result = np.genfromtxt('NUT - ML_DL/w_history.csv', delimiter=',', encoding='big5')
w_grad_result = np.genfromtxt('NUT - ML_DL/w_grad_history.csv', delimiter=',', encoding='big5')

print('Model : y = x * w + b')
# predict the PM2.5 of training data

Average_ERROR = 10000.0  # a random initial value for average error comparision
t_start = time.time()
for c in range(0, len(b_history), 50):    # to find the index, which its average error is the smallest.
  prediction_train = np.array([])
  for i in range(len(train_x)):
    predict_value = sum(train_x[i] * w_history[c]) + b_history[c]
    if(predict_value < 0):  # when prediction is a negative number, set it to 0.0
      predict_value = 0.0
    prediction_train = np.append(prediction_train, predict_value)
  ERROR = abs(sum(prediction_train - train_y))
  if (ERROR < Average_ERROR):  # if current error is smaller, then update the average error.
    Average_ERROR = ERROR
    result_index = c
  if (c % 500 == 0):
    t_end = time.time()
    print('-\nIteration %d: Current Best Average_ERROR = %.5f' % (c, Average_ERROR))
    print('Elapsed time: %.2f min.' % ((t_end - t_start) / 60.0))
print('-\nFinal index %d, with smallest AVERAGE ERROR : %.5f' % (result_index, Average_ERROR))
b_final = b_history[result_index]  # the best-performed b value
w_final = w_history[result_index]  # the best-performed w value

prediction_train

print('Model : y = x * w + b')
# predict the PM2.5 of training data
prediction_train = np.array([])
for i in range(len(train_x)):
  predict_value = sum(train_x[i] * w_final) + b_final
  if(predict_value < 0):
    predict_value = 0.0
  prediction_train = np.append(prediction_train, predict_value)

print('Average ERROR on Training Data :', sum(prediction_train - train_y))
training_data_prediction = prediction_train

# Collect testing data form test.csv
test_data = np.genfromtxt('test.csv', delimiter=',', encoding='big5')
test = test_data[ : , 2 : ]

for i in range(test.shape[0]):
  for j in range(test.shape[1]):
    if(math.isnan(test[i][j])):
      test[i][j] = 0.0

test_data = [ ]
daily_data = np.array([])
i = 0
while(i < len(test)):  
  daily_data = np.append(daily_data, test[i])  
  if(i % 18 == 17):
    test_data.append(daily_data.reshape(18 * 9))
    daily_data = np.array([])
  i += 1

test_data = np.array(test_data)
print('Shape of test_data :', test_data.shape)   # 162 = 18 * 9

# Nolmalization
for i in range(test_data.shape[0]):
  for j in range(test_data.shape[1]):
    if not std[j] == 0 :
      test_data[i][j] = (test_data[i][j]- mean[j]) / std[j]

print('Model : y = x * w + b')
b_final = b  # the best-performed b value
w_final = w  # the best-performed w value

# predict the 10-th-hour PM2.5 of test data
test_result = np.array([])
testing_data_prediction = test_data.dot(w_final) + b_final

# write the prediction into file
with open('submission.csv', 'w', newline='') as csvfile:
  writer = csv.writer(csvfile)
  writer.writerow(['id', 'value'])
  for i in range(len(testing_data_prediction)):
    writer.writerow(['id_' + str(i), testing_data_prediction[i][0]])

testing_data_prediction

genre = 1
for i in range(len(w)):
  if(i % 9 == 0):
    print('-\ngenre', genre, ':')
    print('mean =', abs(np.mean(w[i : i + 9])))
    genre += 1
  print('i =', i, ', w =', w[i])

for row in range(0, 18):
  print(row)

